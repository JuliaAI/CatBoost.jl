var documenterSearchIndex = {"docs":
[{"location":"wrapper/#Python-Wrapper","page":"Wrapper","title":"Python Wrapper","text":"","category":"section"},{"location":"wrapper/","page":"Wrapper","title":"Wrapper","text":"Below is the Python wrapper documentation for CatBoost.jl.","category":"page"},{"location":"wrapper/","page":"Wrapper","title":"Wrapper","text":"Pool\nCatBoost.CatBoostClassifier\nCatBoost.CatBoostRegressor\ncv\nto_catboost\nto_pandas\npandas_to_tbl\nfeature_importance\nload_dataset","category":"page"},{"location":"wrapper/#CatBoost.Pool","page":"Wrapper","title":"CatBoost.Pool","text":"Pool(data; label=nothing, cat_features=nothing, text_features=nothing,\n        pairs=nothing, delimiter='\t', has_header=false, weight=nothing,\n        group_id = nothing, group_weight=nothing, subgroup_id=nothing,\n        pairs_weight=nothing, baseline=nothing, features_names=nothing,\n        thread_count = -1) -> Py\n\nCreates a Pool object holding training data and labels. data may also be passed as a keyword argument.\n\n\n\nPython documentation for catboost.Pool\n\nPython class Pool.\n\nPool used in CatBoost as a data structure to train model from.\n\n\n\n\n\n","category":"function"},{"location":"wrapper/#CatBoost.CatBoostClassifier","page":"Wrapper","title":"CatBoost.CatBoostClassifier","text":"CatBoostClassifier(args...; kwargs...) -> Py\n\nCreates a CatBoostClassifier object.\n\n\n\nPython documentation for catboost.CatBoostClassifier\n\nPython class CatBoostClassifier.\n\nImplementation of the scikit-learn API for CatBoost classification.\n\nParameters\n----------\niterations : int, [default=500]\n    Max count of trees.\n    range: [1,+inf]\nlearning_rate : float, [default value is selected automatically for binary classification with other parameters set to default. In all other cases default is 0.03]\n    Step size shrinkage used in update to prevents overfitting.\n    range: (0,1]\ndepth : int, [default=6]\n    Depth of a tree. All trees are the same depth.\n    range: [1,+inf]\nl2_leaf_reg : float, [default=3.0]\n    Coefficient at the L2 regularization term of the cost function.\n    range: [0,+inf]\nmodel_size_reg : float, [default=None]\n    Model size regularization coefficient.\n    range: [0,+inf]\nrsm : float, [default=None]\n    Subsample ratio of columns when constructing each tree.\n    range: (0,1]\nloss_function : string or object, [default='Logloss']\n    The metric to use in training and also selector of the machine learning\n    problem to solve. If string, then the name of a supported metric,\n    optionally suffixed with parameter description.\n    If object, it shall provide methods 'calc_ders_range' or 'calc_ders_multi'.\nborder_count : int, [default = 254 for training on CPU or 128 for training on GPU]\n    The number of partitions in numeric features binarization. Used in the preliminary calculation.\n    range: [1,65535] on CPU, [1,255] on GPU\nfeature_border_type : string, [default='GreedyLogSum']\n    The binarization mode in numeric features binarization. Used in the preliminary calculation.\n    Possible values:\n        - 'Median'\n        - 'Uniform'\n        - 'UniformAndQuantiles'\n        - 'GreedyLogSum'\n        - 'MaxLogSum'\n        - 'MinEntropy'\nper_float_feature_quantization : list of strings, [default=None]\n    List of float binarization descriptions.\n    Format : described in documentation on catboost.ai\n    Example 1: ['0:1024'] means that feature 0 will have 1024 borders.\n    Example 2: ['0:border_count=1024', '1:border_count=1024', ...] means that two first features have 1024 borders.\n    Example 3: ['0:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum',\n                '1:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum'] - defines more quantization properties for first two features.\ninput_borders : string or pathlib.Path, [default=None]\n    input file with borders used in numeric features binarization.\noutput_borders : string, [default=None]\n    output file for borders that were used in numeric features binarization.\nfold_permutation_block : int, [default=1]\n    To accelerate the learning.\n    The recommended value is within [1, 256]. On small samples, must be set to 1.\n    range: [1,+inf]\nod_pval : float, [default=None]\n    Use overfitting detector to stop training when reaching a specified threshold.\n    Can be used only with eval_set.\n    range: [0,1]\nod_wait : int, [default=None]\n    Number of iterations which overfitting detector will wait after new best error.\nod_type : string, [default=None]\n    Type of overfitting detector which will be used in program.\n    Posible values:\n        - 'IncToDec'\n        - 'Iter'\n    For 'Iter' type od_pval must not be set.\n    If None, then od_type=IncToDec.\nnan_mode : string, [default=None]\n    Way to process missing values for numeric features.\n    Possible values:\n        - 'Forbidden' - raises an exception if there is a missing value for a numeric feature in a dataset.\n        - 'Min' - each missing value will be processed as the minimum numerical value.\n        - 'Max' - each missing value will be processed as the maximum numerical value.\n    If None, then nan_mode=Min.\ncounter_calc_method : string, [default=None]\n    The method used to calculate counters for dataset with Counter type.\n    Possible values:\n        - 'PrefixTest' - only objects up to current in the test dataset are considered\n        - 'FullTest' - all objects are considered in the test dataset\n        - 'SkipTest' - Objects from test dataset are not considered\n        - 'Full' - all objects are considered for both learn and test dataset\n    If None, then counter_calc_method=PrefixTest.\nleaf_estimation_iterations : int, [default=None]\n    The number of steps in the gradient when calculating the values in the leaves.\n    If None, then leaf_estimation_iterations=1.\n    range: [1,+inf]\nleaf_estimation_method : string, [default=None]\n    The method used to calculate the values in the leaves.\n    Possible values:\n        - 'Newton'\n        - 'Gradient'\nthread_count : int, [default=None]\n    Number of parallel threads used to run CatBoost.\n    If None or -1, then the number of threads is set to the number of CPU cores.\n    range: [1,+inf]\nrandom_seed : int, [default=None]\n    Random number seed.\n    If None, 0 is used.\n    range: [0,+inf]\nuse_best_model : bool, [default=None]\n    To limit the number of trees in predict() using information about the optimal value of the error function.\n    Can be used only with eval_set.\nbest_model_min_trees : int, [default=None]\n    The minimal number of trees the best model should have.\nverbose: bool\n    When set to True, logging_level is set to 'Verbose'.\n    When set to False, logging_level is set to 'Silent'.\nsilent: bool, synonym for verbose\nlogging_level : string, [default='Verbose']\n    Possible values:\n        - 'Silent'\n        - 'Verbose'\n        - 'Info'\n        - 'Debug'\nmetric_period : int, [default=1]\n    The frequency of iterations to print the information to stdout. The value should be a positive integer.\nsimple_ctr: list of strings, [default=None]\n    Binarization settings for categorical features.\n        Format : see documentation\n        Example: ['Borders:CtrBorderCount=5:Prior=0:Prior=0.5', 'BinarizedTargetMeanValue:TargetBorderCount=10:TargetBorderType=MinEntropy', ...]\n        CTR types:\n            CPU and GPU\n            - 'Borders'\n            - 'Buckets'\n            CPU only\n            - 'BinarizedTargetMeanValue'\n            - 'Counter'\n            GPU only\n            - 'FloatTargetMeanValue'\n            - 'FeatureFreq'\n        Number_of_borders, binarization type, target borders and binarizations, priors are optional parametrs\ncombinations_ctr: list of strings, [default=None]\nper_feature_ctr: list of strings, [default=None]\nctr_target_border_count: int, [default=None]\n    Maximum number of borders used in target binarization for categorical features that need it.\n    If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it\n    overrides this value.\n    range: [1, 255]\nctr_leaf_count_limit : int, [default=None]\n    The maximum number of leaves with categorical features.\n    If the number of leaves exceeds the specified limit, some leaves are discarded.\n    The leaves to be discarded are selected as follows:\n        - The leaves are sorted by the frequency of the values.\n        - The top N leaves are selected, where N is the value specified in the parameter.\n        - All leaves starting from N+1 are discarded.\n    This option reduces the resulting model size\n    and the amount of memory required for training.\n    Note that the resulting quality of the model can be affected.\n    range: [1,+inf] (for zero limit use ignored_features)\nstore_all_simple_ctr : bool, [default=None]\n    Ignore categorical features, which are not used in feature combinations,\n    when choosing candidates for exclusion.\n    Use this parameter with ctr_leaf_count_limit only.\nmax_ctr_complexity : int, [default=4]\n    The maximum number of Categ features that can be combined.\n    range: [0,+inf]\nhas_time : bool, [default=False]\n    To use the order in which objects are represented in the input data\n    (do not perform a random permutation of the dataset at the preprocessing stage).\nallow_const_label : bool, [default=False]\n    To allow the constant label value in dataset.\ntarget_border: float, [default=None]\n    Border for target binarization.\nclasses_count : int, [default=None]\n    The upper limit for the numeric class label.\n    Defines the number of classes for multiclassification.\n    Only non-negative integers can be specified.\n    The given integer should be greater than any of the target values.\n    If this parameter is specified the labels for all classes in the input dataset\n    should be smaller than the given value.\n    If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n    the numbers of classes specified by each of them must be equal.\nclass_weights : list or dict, [default=None]\n    Classes weights. The values are used as multipliers for the object weights.\n    If None, all classes are supposed to have weight one.\n    If list - class weights in order of class_names or sequential classes if class_names is undefined\n    If dict - dict of class_name -> class_weight.\n    If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n    the numbers of classes specified by each of them must be equal.\nauto_class_weights : string [default=None]\n    Enables automatic class weights calculation. Possible values:\n        - Balanced  # weight = maxSummaryClassWeight / summaryClassWeight, statistics determined from train pool\n        - SqrtBalanced  # weight = sqrt(maxSummaryClassWeight / summaryClassWeight)\nclass_names: list of strings, [default=None]\n    Class names. Allows to redefine the default values for class labels (integer numbers).\n    If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n    the numbers of classes specified by each of them must be equal.\none_hot_max_size : int, [default=None]\n    Convert the feature to float\n    if the number of different values that it takes exceeds the specified value.\n    Ctrs are not calculated for such features.\nrandom_strength : float, [default=1]\n    Score standard deviation multiplier.\nname : string, [default='experiment']\n    The name that should be displayed in the visualization tools.\nignored_features : list, [default=None]\n    Indices or names of features that should be excluded when training.\ntrain_dir : string or pathlib.Path, [default=None]\n    The directory in which you want to record generated in the process of learning files.\ncustom_metric : string or list of strings, [default=None]\n    To use your own metric function.\ncustom_loss: alias to custom_metric\neval_metric : string or object, [default=None]\n    To optimize your custom metric in loss.\nbagging_temperature : float, [default=None]\n    Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.\n    Typical values are in range [0, 1] (0 - no bagging, 1 - default).\nsave_snapshot : bool, [default=None]\n    Enable progress snapshotting for restoring progress after crashes or interruptions\nsnapshot_file : string or pathlib.Path, [default=None]\n    Learn progress snapshot file path, if None will use default filename\nsnapshot_interval: int, [default=600]\n    Interval between saving snapshots (seconds)\nfold_len_multiplier : float, [default=None]\n    Fold length multiplier. Should be greater than 1\nused_ram_limit : string or number, [default=None]\n    Set a limit on memory consumption (value like '1.2gb' or 1.2e9).\n    WARNING: Currently this option affects CTR memory usage only.\ngpu_ram_part : float, [default=0.95]\n    Fraction of the GPU RAM to use for training, a value from (0, 1].\npinned_memory_size: int [default=None]\n    Size of additional CPU pinned memory used for GPU learning,\n    usually is estimated automatically, thus usually should not be set.\nallow_writing_files : bool, [default=True]\n    If this flag is set to False, no files with different diagnostic info will be created during training.\n    With this flag no snapshotting can be done. Plus visualisation will not\n    work, because visualisation uses files that are created and updated during training.\nfinal_ctr_computation_mode : string, [default='Default']\n    Possible values:\n        - 'Default' - Compute final ctrs for all pools.\n        - 'Skip' - Skip final ctr computation. WARNING: model without ctrs can't be applied.\napprox_on_full_history : bool, [default=False]\n    If this flag is set to True, each approximated value is calculated using all the preceeding rows in the fold (slower, more accurate).\n    If this flag is set to False, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).\nboosting_type : string, default value depends on object count and feature count in train dataset and on learning mode.\n    Boosting scheme.\n    Possible values:\n        - 'Ordered' - Gives better quality, but may slow down the training.\n        - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.\ntask_type : string, [default=None]\n    The calcer type used to train the model.\n    Possible values:\n        - 'CPU'\n        - 'GPU'\ndevice_config : string, [default=None], deprecated, use devices instead\ndevices : list or string, [default=None], GPU devices to use.\n    String format is: '0' for 1 device or '0:1:3' for multiple devices or '0-3' for range of devices.\n    List format is : [0] for 1 device or [0,1,3] for multiple devices.\n\nbootstrap_type : string, Bayesian, Bernoulli, Poisson, MVS.\n    Default bootstrap is Bayesian for GPU and MVS for CPU.\n    Poisson bootstrap is supported only on GPU.\n    MVS bootstrap is supported only on CPU.\n\nsubsample : float, [default=None]\n    Sample rate for bagging. This parameter can be used Poisson or Bernoully bootstrap types.\n\nmvs_reg : float, [default is set automatically at each iteration based on gradient distribution]\n    Regularization parameter for MVS sampling algorithm\n\nmonotone_constraints : list or numpy.ndarray or string or dict, [default=None]\n    Monotone constraints for features.\n\nfeature_weights : list or numpy.ndarray or string or dict, [default=None]\n    Coefficient to multiply split gain with specific feature use. Should be non-negative.\n\npenalties_coefficient : float, [default=1]\n    Common coefficient for all penalties. Should be non-negative.\n\nfirst_feature_use_penalties : list or numpy.ndarray or string or dict, [default=None]\n    Penalties to first use of specific feature in model. Should be non-negative.\n\nper_object_feature_penalties : list or numpy.ndarray or string or dict, [default=None]\n    Penalties for first use of feature for each object. Should be non-negative.\n\nsampling_frequency : string, [default=PerTree]\n    Frequency to sample weights and objects when building trees.\n    Possible values:\n        - 'PerTree' - Before constructing each new tree\n        - 'PerTreeLevel' - Before choosing each new split of a tree\n\nsampling_unit : string, [default='Object'].\n    Possible values:\n        - 'Object'\n        - 'Group'\n    The parameter allows to specify the sampling scheme:\n    sample weights for each object individually or for an entire group of objects together.\n\ndev_score_calc_obj_block_size: int, [default=5000000]\n    CPU only. Size of block of samples in score calculation. Should be > 0\n    Used only for learning speed tuning.\n    Changing this parameter can affect results due to numerical accuracy differences\n\ndev_efb_max_buckets : int, [default=1024]\n    CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.\n    Used only for learning speed tuning.\n\nsparse_features_conflict_fraction : float, [default=0.0]\n    CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.\n    Should be a real value in [0, 1) interval.\n\ngrow_policy : string, [SymmetricTree,Lossguide,Depthwise], [default=SymmetricTree]\n    The tree growing policy. It describes how to perform greedy tree construction.\n\nmin_data_in_leaf : int, [default=1].\n    The minimum training samples count in leaf.\n    CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.\n    This parameter is used only for Depthwise and Lossguide growing policies.\n\nmax_leaves : int, [default=31],\n    The maximum leaf count in resulting tree.\n    This parameter is used only for Lossguide growing policy.\n\nscore_function : string, possible values L2, Cosine, NewtonL2, NewtonCosine, [default=Cosine]\n    For growing policy Lossguide default=NewtonL2.\n    GPU only. Score that is used during tree construction to select the next tree split.\n\nmax_depth : int, Synonym for depth.\n\nn_estimators : int, synonym for iterations.\n\nnum_trees : int, synonym for iterations.\n\nnum_boost_round : int, synonym for iterations.\n\ncolsample_bylevel : float, synonym for rsm.\n\nrandom_state : int, synonym for random_seed.\n\nreg_lambda : float, synonym for l2_leaf_reg.\n\nobjective : string, synonym for loss_function.\n\nnum_leaves : int, synonym for max_leaves.\n\nmin_child_samples : int, synonym for min_data_in_leaf\n\neta : float, synonym for learning_rate.\n\nmax_bin : float, synonym for border_count.\n\nscale_pos_weight : float, synonym for class_weights.\n    Can be used only for binary classification. Sets weight multiplier for\n    class 1 to scale_pos_weight value.\n\nmetadata : dict, string to string key-value pairs to be stored in model metadata storage\n\nearly_stopping_rounds : int\n    Synonym for od_wait. Only one of these parameters should be set.\n\ncat_features : list or numpy.ndarray, [default=None]\n    If not None, giving the list of Categ features indices or names (names are represented as strings).\n    If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n\ntext_features : list or numpy.ndarray, [default=None]\n    If not None, giving the list of Text features indices or names (names are represented as strings).\n    If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n\nembedding_features : list or numpy.ndarray, [default=None]\n    If not None, giving the list of Embedding features indices or names (names are represented as strings).\n    If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n\nleaf_estimation_backtracking : string, [default=None]\n    Type of backtracking during gradient descent.\n    Possible values:\n        - 'No' - never backtrack; supported on CPU and GPU\n        - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU\n        - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only\n\nmodel_shrink_rate : float, [default=0]\n    This parameter enables shrinkage of model at the start of each iteration. CPU only.\n    For Constant mode shrinkage coefficient is calculated as (1 - model_shrink_rate * learning_rate).\n    For Decreasing mode shrinkage coefficient is calculated as (1 - model_shrink_rate / iteration).\n    Shrinkage coefficient should be in [0, 1).\n\nmodel_shrink_mode : string, [default=None]\n    Mode of shrinkage coefficient calculation. CPU only.\n    Possible values:\n        - 'Constant' - Shrinkage coefficient is constant at each iteration.\n        - 'Decreasing' - Shrinkage coefficient decreases at each iteration.\n\nlangevin : bool, [default=False]\n    Enables the Stochastic Gradient Langevin Boosting. CPU only.\n\ndiffusion_temperature : float, [default=0]\n    Langevin boosting diffusion temperature. CPU only.\n\nposterior_sampling : bool, [default=False]\n    Set group of parameters for further use Uncertainty prediction:\n        - Langevin = True\n        - Model Shrink Rate = 1/(2N), where N is dataset size\n        - Model Shrink Mode = Constant\n        - Diffusion-temperature = N, where N is dataset size. CPU only.\n\nboost_from_average : bool, [default=True for RMSE, False for other losses]\n    Enables to initialize approx values by best constant value for specified loss function.\n    Available for RMSE, Logloss, CrossEntropy, Quantile and MAE.\n\ntokenizers : list of dicts,\n    Each dict is a tokenizer description. Example:\n    ```\n    [\n        {\n            'tokenizer_id': 'Tokenizer',  # Tokeinzer identifier.\n            'lowercasing': 'false',  # Possible values: 'true', 'false'.\n            'number_process_policy': 'LeaveAsIs',  # Possible values: 'Skip', 'LeaveAsIs', 'Replace'.\n            'number_token': '%',  # Rarely used character. Used in conjunction with Replace NumberProcessPolicy.\n            'separator_type': 'ByDelimiter',  # Possible values: 'ByDelimiter', 'BySense'.\n            'delimiter': ' ',  # Used in conjunction with ByDelimiter SeparatorType.\n            'split_by_set': 'false',  # Each single character in delimiter used as individual delimiter.\n            'skip_empty': 'true',  # Possible values: 'true', 'false'.\n            'token_types': ['Word', 'Number', 'Unknown'],  # Used in conjunction with BySense SeparatorType.\n                # Possible values: 'Word', 'Number', 'Punctuation', 'SentenceBreak', 'ParagraphBreak', 'Unknown'.\n            'subtokens_policy': 'SingleToken',  # Possible values:\n                # 'SingleToken' - All subtokens are interpreted as single token).\n                # 'SeveralTokens' - All subtokens are interpreted as several token.\n        },\n        ...\n    ]\n    ```\n\ndictionaries : list of dicts,\n    Each dict is a tokenizer description. Example:\n    ```\n    [\n        {\n            'dictionary_id': 'Dictionary',  # Dictionary identifier.\n            'token_level_type': 'Word',  # Possible values: 'Word', 'Letter'.\n            'gram_order': '1',  # 1 for Unigram, 2 for Bigram, ...\n            'skip_step': '0',  # 1 for 1-skip-gram, ...\n            'end_of_word_token_policy': 'Insert',  # Possible values: 'Insert', 'Skip'.\n            'end_of_sentence_token_policy': 'Skip',  # Possible values: 'Insert', 'Skip'.\n            'occurrence_lower_bound': '3',  # The lower bound of token occurrences in the text to include it in the dictionary.\n            'max_dictionary_size': '50000',  # The max dictionary size.\n        },\n        ...\n    ]\n    ```\n\nfeature_calcers : list of strings,\n    Each string is a calcer description. Example:\n    ```\n    [\n        'NaiveBayes',\n        'BM25',\n        'BoW:top_tokens_count=2000',\n    ]\n    ```\n\ntext_processing : dict,\n    Text processging description.\n    \neval_fraction : float, [default=None]\n    Fraction of the train dataset to be used as the evaluation dataset.\n\n\n\n\n\n","category":"function"},{"location":"wrapper/#CatBoost.CatBoostRegressor","page":"Wrapper","title":"CatBoost.CatBoostRegressor","text":"CatBoostRegressor(args...; kwargs...) -> Py\n\nCreates a CatBoostRegressor object.\n\n\n\nPython documentation for catboost.CatBoostRegressor\n\nPython class CatBoostRegressor.\n\nImplementation of the scikit-learn API for CatBoost regression.\n\nParameters\n----------\nLike in CatBoostClassifier, except loss_function, classes_count, class_names and class_weights\n\nloss_function : string, [default='RMSE']\n    'RMSE'\n    'MAE'\n    'Quantile:alpha=value'\n    'LogLinQuantile:alpha=value'\n    'Poisson'\n    'MAPE'\n    'Lq:q=value'\n    'SurvivalAft:dist=value;scale=value'\n\n\n\n\n\n","category":"function"},{"location":"wrapper/#CatBoost.cv","page":"Wrapper","title":"CatBoost.cv","text":"cv(pool::Py; kwargs...) -> Table\n\nAccepts a CatBoost.Pool positional argument to specify the training data, and keyword arguments to configure the settings. See the python documentation below for what keyword arguments are accepted.\n\n\n\nPython documentation for catboost.cv\n\nPython function cv.\n\nCross-validate the CatBoost model.\n\nParameters\n----------\npool : catboost.Pool\n    Data to cross-validate on.\n\nparams : dict\n    Parameters for CatBoost.\n    CatBoost has many of parameters, all have default values.\n    If  None, all params still defaults.\n    If  dict, overriding some (or all) params.\n\ndtrain : catboost.Pool or tuple (X, y)\n    Synonym for pool parameter. Only one of these parameters should be set.\n\niterations : int\n    Number of boosting iterations. Can be set in params dict.\n\nnum_boost_round : int\n    Synonym for iterations. Only one of these parameters should be set.\n\nfold_count : int, optional (default=3)\n    The number of folds to split the dataset into.\n\nnfold : int\n    Synonym for fold_count.\n\ntype : string, optional (default='Classical')\n    Type of cross-validation\n    Possible values:\n        - 'Classical'\n        - 'Inverted'\n        - 'TimeSeries'\n\ninverted : bool, optional (default=False)\n    Train on the test fold and evaluate the model on the training folds.\n\npartition_random_seed : int, optional (default=0)\n    Use this as the seed value for random permutation of the data.\n    Permutation is performed before splitting the data for cross validation.\n    Each seed generates unique data splits.\n\nseed : int, optional\n    Synonym for partition_random_seed. This parameter is deprecated. Use\n    partition_random_seed instead.\n    If both parameters are initialised partition_random_seed parameter is\n    ignored.\n\nshuffle : bool, optional (default=True)\n    Shuffle the dataset objects before splitting into folds.\n\nlogging_level : string, optional (default=None)\n    Possible values:\n        - 'Silent'\n        - 'Verbose'\n        - 'Info'\n        - 'Debug'\n\nstratified : bool, optional (default=None)\n    Perform stratified sampling. True for classification and False otherwise.\n\nas_pandas : bool, optional (default=True)\n    Return pd.DataFrame when pandas is installed.\n    If False or pandas is not installed, return dict.\n\nmetric_period : int, [default=1]\n    The frequency of iterations to print the information to stdout. The value should be a positive integer.\n\nverbose : bool or int\n    If verbose is bool, then if set to True, logging_level is set to Verbose,\n    if set to False, logging_level is set to Silent.\n    If verbose is int, it determines the frequency of writing metrics to output and\n    logging_level is set to Verbose.\n\nverbose_eval : bool or int\n    Synonym for verbose. Only one of these parameters should be set.\n\nplot : bool, optional (default=False)\n    If True, draw train and eval error in Jupyter notebook\n\nplot_file : file-like or str, optional (default=None)\n    If not None, save train and eval error graphs to file\n\nearly_stopping_rounds : int\n    Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n\nsave_snapshot : bool, [default=None]\n    Enable progress snapshotting for restoring progress after crashes or interruptions\n\nsnapshot_file : string or pathlib.Path, [default=None]\n    Learn progress snapshot file path, if None will use default filename\n\nsnapshot_interval: int, [default=600]\n    Interval between saving snapshots (seconds)\n\nmetric_update_interval: float, [default=0.5]\n    Interval between updating metrics (seconds)\n\nfolds: generator or iterator of (train_idx, test_idx) tuples, scikit-learn splitter object or None, optional (default=None)\n    If generator or iterator, it should yield the train and test indices for each fold.\n    If object, it should be one of the scikit-learn splitter classes\n    (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n    and have ``split`` method.\n    if folds is not None, then all of fold_count, shuffle, partition_random_seed, inverted are None\n\nreturn_models: bool, optional (default=False)\n    if True, return a list of models fitted for each CV fold\n\nlog_cout: output stream or callback for logging\n\nlog_cerr: error stream or callback for logging\n\nReturns\n-------\ncv results : pandas.core.frame.DataFrame with cross-validation results\n    columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\ncv models : list of trained models, if return_models=True\n\n\n\n\n\n","category":"function"},{"location":"wrapper/#CatBoost.pandas_to_tbl","page":"Wrapper","title":"CatBoost.pandas_to_tbl","text":"pandas_to_tbl(pandas_df::Py)\n\nConvert a pandas dataframe into a Tables.jl columntable\n\n\n\n\n\n","category":"function"},{"location":"wrapper/#CatBoost.load_dataset","page":"Wrapper","title":"CatBoost.load_dataset","text":"load_dataset(dataset_name::Symbol)\n\nImport a catboost dataset\n\n\n\n\n\n","category":"function"},{"location":"mlj_api/#MLJ-API","page":"MLJ API","title":"MLJ API","text":"","category":"section"},{"location":"mlj_api/","page":"MLJ API","title":"MLJ API","text":"Below is the MLJ API documentation for CatBoost.jl.","category":"page"},{"location":"mlj_api/","page":"MLJ API","title":"MLJ API","text":"CatBoost.MLJCatBoostInterface.CatBoostClassifier\nCatBoost.MLJCatBoostInterface.CatBoostRegressor","category":"page"},{"location":"mlj_api/#CatBoost.MLJCatBoostInterface.CatBoostClassifier","page":"MLJ API","title":"CatBoost.MLJCatBoostInterface.CatBoostClassifier","text":"CatBoostClassifier\n\nA model type for constructing a CatBoost classifier, based on CatBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCatBoostClassifier = @load CatBoostClassifier pkg=CatBoost\n\nDo model = CatBoostClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CatBoostClassifier(iterations=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, Finite, Textual; check column scitypes with schema(X). Textual columns will be passed to catboost as text_features, Multiclass columns will be passed to catboost as cat_features, and OrderedFactor columns will be converted to integers.\ny: the target, which can be any AbstractVector whose element scitype is Finite; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\nOperations\n\npredict(mach, Xnew): probabilistic predictions of the target given new features Xnew having the same scitype as X above.\npredict_mode(mach, Xnew): returns the mode of each of the prediction above.\n\nAccessor functions\n\nfeature_importances(mach): return vector of feature importances, in the form of   feature::Symbol => importance::Real pairs\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: The Python CatBoostClassifier model\n\nReport\n\nThe fields of report(mach) are:\n\nfeature_importances: Vector{Pair{Symbol, Float64}} of feature importances\n\nExamples\n\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = coerce([0, 0, 1, 1], Multiclass)\n\nmodel = CatBoostClassifier(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\nprobs = predict(mach, X)\npreds = predict_mode(mach, X)\n\nSee also catboost and the unwrapped model type CatBoost.CatBoostClassifier.\n\n\n\n\n\n","category":"type"},{"location":"mlj_api/#CatBoost.MLJCatBoostInterface.CatBoostRegressor","page":"MLJ API","title":"CatBoost.MLJCatBoostInterface.CatBoostRegressor","text":"CatBoostRegressor\n\nA model type for constructing a CatBoost regressor, based on CatBoost.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nCatBoostRegressor = @load CatBoostRegressor pkg=CatBoost\n\nDo model = CatBoostRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in CatBoostRegressor(iterations=...).\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y)\n\nwhere\n\nX: any table of input features (eg, a DataFrame) whose columns each have one of the following element scitypes: Continuous, Count, Finite, Textual; check column scitypes with schema(X). Textual columns will be passed to catboost as text_features, Multiclass columns will be passed to catboost as cat_features, and OrderedFactor columns will be converted to integers.\ny: the target, which can be any AbstractVector whose element scitype is Continuous; check the scitype with scitype(y)\n\nTrain the machine with fit!(mach, rows=...).\n\nHyper-parameters\n\nMore details on the catboost hyperparameters, here are the Python docs:  https://catboost.ai/en/docs/concepts/python-reference_catboostclassifier#parameters\n\nOperations\n\npredict(mach, Xnew): probabilistic predictions of the target given new features Xnew having the same scitype as X above.\n\nAccessor functions\n\nfeature_importances(mach): return vector of feature importances, in the form of   feature::Symbol => importance::Real pairs\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nmodel: The Python CatBoostRegressor model\n\nReport\n\nThe fields of report(mach) are:\n\nfeature_importances: Vector{Pair{Symbol, Float64}} of feature importances\n\nExamples\n\nusing CatBoost.MLJCatBoostInterface\nusing MLJ\n\nX = (\n    duration = [1.5, 4.1, 5.0, 6.7], \n    n_phone_calls = [4, 5, 6, 7], \n    department = coerce([\"acc\", \"ops\", \"acc\", \"ops\"], Multiclass), \n)\ny = [2.0, 4.0, 6.0, 7.0]\n\nmodel = CatBoostRegressor(iterations=5)\nmach = machine(model, X, y)\nfit!(mach)\npreds = predict(mach, X)\n\nSee also catboost and the unwrapped model type CatBoost.CatBoostRegressor.\n\n\n\n\n\n","category":"type"},{"location":"#CatBoost.jl","page":"Introduction","title":"CatBoost.jl","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"Julia interface to CatBoost. This library is a wrapper CatBoost's Python package via PythonCall.jl. ","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"For a nice introduction to the package, see the examples.","category":"page"},{"location":"#Installation","page":"Introduction","title":"Installation","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This package is available in the Julia General Registry. You can install it with either of the following commands:","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"pkg> add CatBoost","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"julia> using Pkg; Pkg.add(\"CatBoost\")","category":"page"}]
}
